{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "hlSCP7UCPeMs",
   "metadata": {
    "id": "hlSCP7UCPeMs"
   },
   "source": [
    "# Introduction - Dataset Creation\n",
    "## Due to unavailibity of a suitable online dataset with grayscale images, we manually created one with respective American Sign Language finger notations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nKNuKeijQugn",
   "metadata": {
    "id": "nKNuKeijQugn"
   },
   "source": [
    "### STEP 1: Import the necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "669e82ce",
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1693470555223,
     "user": {
      "displayName": "Kashvi",
      "userId": "02895682443929248031"
     },
     "user_tz": -330
    },
    "id": "669e82ce"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Flatten, BatchNormalization, Conv2D, MaxPool2D, Dropout\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4b77fe1",
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1693470556601,
     "user": {
      "displayName": "Kashvi",
      "userId": "02895682443929248031"
     },
     "user_tz": -330
    },
    "id": "a4b77fe1"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import cv2\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1aa1e00f",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1693470559459,
     "user": {
      "displayName": "Kashvi",
      "userId": "02895682443929248031"
     },
     "user_tz": -330
    },
    "id": "1aa1e00f"
   },
   "outputs": [],
   "source": [
    "background = None\n",
    "accumulated_weight = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mvDUo3DiRFTe",
   "metadata": {
    "id": "mvDUo3DiRFTe"
   },
   "source": [
    "### STEP 2: Define dimensions for the Region of Interest (ROI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfb7c5ba",
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1693470559461,
     "user": {
      "displayName": "Kashvi",
      "userId": "02895682443929248031"
     },
     "user_tz": -330
    },
    "id": "dfb7c5ba"
   },
   "outputs": [],
   "source": [
    "#Creating the dimensions for the ROI...\n",
    "ROI_top = 100\n",
    "ROI_bottom = 300\n",
    "ROI_right = 150\n",
    "ROI_left = 350"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c082cda7",
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1693470560618,
     "user": {
      "displayName": "Kashvi",
      "userId": "02895682443929248031"
     },
     "user_tz": -330
    },
    "id": "c082cda7"
   },
   "outputs": [],
   "source": [
    "def cal_accum_avg(frame, accumulated_weight):\n",
    "    global background\n",
    "\n",
    "    if background is None:\n",
    "        background = frame.copy().astype(\"float\")\n",
    "        return None\n",
    "    cv2.accumulateWeighted(frame, background, accumulated_weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LqS6--dHRQ7-",
   "metadata": {
    "id": "LqS6--dHRQ7-"
   },
   "source": [
    "***cal_accum_avg*** helps us perform background subtraction calculations to display a proper frame for sign recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cb52f8d",
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1693470563649,
     "user": {
      "displayName": "Kashvi",
      "userId": "02895682443929248031"
     },
     "user_tz": -330
    },
    "id": "5cb52f8d"
   },
   "outputs": [],
   "source": [
    "def segment_hand(frame, threshold=25):\n",
    "    global background\n",
    "\n",
    "    diff = cv2.absdiff(background.astype(\"uint8\"), frame)\n",
    "    _ , thresholded = cv2.threshold(diff, threshold, 255, cv2.THRESH_BINARY)\n",
    "    contours, hierarchy = cv2.findContours(thresholded.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    if contours is not None and len(contours) > 0:\n",
    "        hand_segment_max_cont = max(contours, key=cv2.contourArea)\n",
    "        return (thresholded, hand_segment_max_cont)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9Z1G51BYRrzi",
   "metadata": {
    "id": "9Z1G51BYRrzi"
   },
   "source": [
    "***segment_hand*** uses contouring to separate out the signalling hand from the frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2B4udaVgR8DF",
   "metadata": {
    "id": "2B4udaVgR8DF"
   },
   "source": [
    "### STEP 3: Prepare the Training Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7fd220ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "error",
     "timestamp": 1693470649185,
     "user": {
      "displayName": "Kashvi",
      "userId": "02895682443929248031"
     },
     "user_tz": -330
    },
    "id": "7fd220ed",
    "outputId": "fde425f4-30a6-4e63-974a-786748ec1cf3"
   },
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "num_frames = 0\n",
    "element = 8\n",
    "num_imgs_taken = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    # flipping the frame to prevent inverted image of captured frame...\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame_copy = frame.copy()\n",
    "    roi = frame[ROI_top:ROI_bottom, ROI_right:ROI_left]\n",
    "    gray_frame = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    gray_frame = cv2.GaussianBlur(gray_frame, (9, 9), 0)\n",
    "    if num_frames < 60:\n",
    "        cal_accum_avg(gray_frame, accumulated_weight)\n",
    "        if num_frames <= 59:\n",
    "\n",
    "            cv2.putText(frame_copy, \"FETCHING BACKGROUND...PLEASE WAIT\",(80, 400), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,255), 2)\n",
    "\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Segmenting the hand region...\n",
    "        hand = segment_hand(gray_frame)\n",
    "\n",
    "        # Checking if we are able to detect the hand...\n",
    "        if hand is not None:\n",
    "\n",
    "            # unpack the thresholded img and the max_contour...\n",
    "            thresholded, hand_segment = hand\n",
    "            # Drawing contours around hand segment\n",
    "            cv2.drawContours(frame_copy, [hand_segment + (ROI_right,ROI_top)], -1, (255, 0, 0),1)\n",
    "\n",
    "            cv2.putText(frame_copy, str(num_frames), (70, 45),cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "\n",
    "            cv2.putText(frame_copy, str(num_imgs_taken) + 'images' +\"For\"+ str(element), (200, 400), cv2.FONT_HERSHEY_SIMPLEX, 1,(0,0,255), 2)\n",
    "\n",
    "            # Displaying the thresholded image\n",
    "            cv2.imshow(\"Thresholded Hand Image\", thresholded)\n",
    "            if num_imgs_taken <= 400:\n",
    "                cv2.imwrite(r\"D:\\\\INTEL\\\\gesture\\\\train\\\\\"+str(element)+\"\\\\\" +\n",
    "                str(num_imgs_taken+300) + '.jpg', thresholded)\n",
    "\n",
    "            else:\n",
    "                break\n",
    "            num_imgs_taken +=1\n",
    "        else:\n",
    "            cv2.putText(frame_copy, 'No hand detected...', (200, 400),\n",
    " cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "    # Drawing ROI on frame copy\n",
    "    cv2.rectangle(frame_copy, (ROI_left, ROI_top), (ROI_right,ROI_bottom), (255,128,0), 3)\n",
    "\n",
    "    cv2.putText(frame_copy, \"DataFlair hand sign recognition_ _ _\", (10, 20), cv2.FONT_ITALIC, 0.5, (51,255,51), 1)\n",
    "\n",
    "    # increment the number of frames for tracking\n",
    "    num_frames += 1\n",
    "    # Display the frame with segmented hand\n",
    "    cv2.imshow(\"Sign Detection\", frame_copy)\n",
    "    # Closing windows with Esc key...(any other key with ord can be used too.)\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "# Releasing the camera & destroying all the windows...\n",
    "cv2.destroyAllWindows()\n",
    "cam.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jC0RDIh6SEjW",
   "metadata": {
    "id": "jC0RDIh6SEjW"
   },
   "source": [
    "### STEP 4: Prepare the Testing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4240eeca",
   "metadata": {
    "id": "4240eeca"
   },
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "num_frames = 0\n",
    "element = 8\n",
    "num_imgs_taken = 0\n",
    "\n",
    "while True:\n",
    "    ret, frame = cam.read()\n",
    "    # flipping the frame to prevent inverted image of captured frame...\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    frame_copy = frame.copy()\n",
    "    roi = frame[ROI_top:ROI_bottom, ROI_right:ROI_left]\n",
    "    gray_frame = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    gray_frame = cv2.GaussianBlur(gray_frame, (9, 9), 0)\n",
    "    if num_frames < 60:\n",
    "        cal_accum_avg(gray_frame, accumulated_weight)\n",
    "        if num_frames <= 59:\n",
    "\n",
    "            cv2.putText(frame_copy, \"FETCHING BACKGROUND...PLEASE WAIT\",(80, 400), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0,0,255), 2)\n",
    "\n",
    "\n",
    "    else:\n",
    "\n",
    "        # Segmenting the hand region...\n",
    "        hand = segment_hand(gray_frame)\n",
    "\n",
    "        # Checking if we are able to detect the hand...\n",
    "        if hand is not None:\n",
    "\n",
    "            # unpack the thresholded img and the max_contour...\n",
    "            thresholded, hand_segment = hand\n",
    "            # Drawing contours around hand segment\n",
    "            cv2.drawContours(frame_copy, [hand_segment + (ROI_right,ROI_top)], -1, (255, 0, 0),1)\n",
    "\n",
    "            cv2.putText(frame_copy, str(num_frames), (70, 45),cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "\n",
    "            cv2.putText(frame_copy, str(num_imgs_taken) + 'images' +\"For\"+ str(element), (200, 400), cv2.FONT_HERSHEY_SIMPLEX, 1,(0,0,255), 2)\n",
    "\n",
    "            # Displaying the thresholded image\n",
    "            cv2.imshow(\"Thresholded Hand Image\", thresholded)\n",
    "            if num_imgs_taken <= 100:\n",
    "                cv2.imwrite(r\"D:\\\\INTEL\\\\gesture\\\\test\\\\\"+str(element)+\"\\\\\" +\n",
    "                str(num_imgs_taken+300) + '.jpg', thresholded)\n",
    "\n",
    "            else:\n",
    "                break\n",
    "            num_imgs_taken +=1\n",
    "        else:\n",
    "            cv2.putText(frame_copy, 'No hand detected...', (200, 400),\n",
    " cv2.FONT_HERSHEY_SIMPLEX, 1, (0,0,255), 2)\n",
    "    # Drawing ROI on frame copy\n",
    "    cv2.rectangle(frame_copy, (ROI_left, ROI_top), (ROI_right,ROI_bottom), (255,128,0), 3)\n",
    "\n",
    "    cv2.putText(frame_copy, \"DataFlair hand sign recognition_ _ _\", (10, 20), cv2.FONT_ITALIC, 0.5, (51,255,51), 1)\n",
    "\n",
    "    # increment the number of frames for tracking\n",
    "    num_frames += 1\n",
    "    # Display the frame with segmented hand\n",
    "    cv2.imshow(\"Sign Detection\", frame_copy)\n",
    "    # Closing windows with Esc key...(any other key with ord can be used too.)\n",
    "    k = cv2.waitKey(1) & 0xFF\n",
    "    if k == 27:\n",
    "        break\n",
    "\n",
    "# Releasing the camera & destroying all the windows...\n",
    "cv2.destroyAllWindows()\n",
    "cam.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BpwpEYXxSvWO",
   "metadata": {
    "id": "BpwpEYXxSvWO"
   },
   "source": [
    "The preparation of respective datasets uses ***opencv*** to capture images and saves them to a pre-existing directory on the running device."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
